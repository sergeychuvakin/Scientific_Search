{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=u\"main.log\", format=u'%(filename)s[LINE:%(lineno)d]# %(levelname)-8s [%(asctime)s]  %(message)s', level=logging.DEBUG)\n",
    "\n",
    "logging.debug( u'debug' )\n",
    "logging.info( u'info' )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import time\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import json\n",
    "# #import Levenshtein as lv\n",
    "# import numpy as np\n",
    "\n",
    "# import spacy\n",
    "\n",
    "\n",
    "# #for findDOI\n",
    "# import feedparser\n",
    "# import jellyfish\n",
    "# import copy\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rss import rss # список рассылки\n",
    "from findDOI import findDOI # функция поиска doi по названию\n",
    "from ALT import altmetrics # функция поиска альтметрик (нужно понять, какие альтметрики нам нужны)\n",
    "from stoplist import stoplist #стоп-лист\n",
    "from rssParser import Parser # класс, который парсит рсс, в датафрейм, - предлагаю его использовать для наполненния\n",
    "from findJournal import findJournal # функция поиска названия журнала по названию статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = Parser(rss)\n",
    "# df = p.main()\n",
    "#df.to_csv('example_table.csv')\n",
    "df = pd.read_csv('example_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>article_name</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Advances in thermochemical conversion of woody...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Fate of degraded pollutants in waste gas biofi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Strain engineering for microbial production of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The state-of-the-art strategies of protein eng...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Engineering of filamentous fungi for efficient...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       article_name  doi journal  \\\n",
       "0           0  Advances in thermochemical conversion of woody...  NaN     NaN   \n",
       "1           1  Fate of degraded pollutants in waste gas biofi...  NaN     NaN   \n",
       "2           2  Strain engineering for microbial production of...  NaN     NaN   \n",
       "3           3  The state-of-the-art strategies of protein eng...  NaN     NaN   \n",
       "4           4  Engineering of filamentous fungi for efficient...  NaN     NaN   \n",
       "\n",
       "                                             summary  \n",
       "0  <p>Publication date: July–August 2019</p><p><b...  \n",
       "1  <p>Publication date: July–August 2019</p><p><b...  \n",
       "2  <p>Publication date: July–August 2019</p><p><b...  \n",
       "3  <p>Publication date: July–August 2019</p><p><b...  \n",
       "4  <p>Publication date: July–August 2019</p><p><b...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Biotechnology Advances'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "findJournal(df.loc[1,][\"article_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', parser=False, ner=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim \n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = '/home/serge/Desktop/local_dev/testmodels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем словарь, векторное пространство, модель Lsi (вектороное пространство)\n",
    "# общий корпус натренированный на pubmed \n",
    "dictionary = corpora.Dictionary.load('/home/serge/Desktop/local_dev/pubmed/pubmed5.dict')      #Here and later - the first one is PubMed-based LSI-object, other - CUB-based LSI-objects\n",
    "corpus = corpora.MmCorpus('/home/serge/Desktop/local_dev/pubmed/pubmed5.mm') \n",
    "lsi = models.LsiModel.load('/home/serge/Desktop/local_dev/pubmed/pubmed5.lsi')\n",
    "\n",
    "# химики куб\n",
    "dictionary_CUB_chem = corpora.Dictionary.load(new_path+\"chem_CUB.dict\")\n",
    "lsi_CUB_chem = models.LsiModel.load(new_path+\"chem_CUB.lsi\")\n",
    "corpus_CUB_chem = corpora.MmCorpus(new_path+\"chem_CUB.mm\")\n",
    "\n",
    "# онко куб\n",
    "dictionary_CUB_onco = corpora.Dictionary.load(new_path+\"cancer_CUB.dict\")\n",
    "lsi_CUB_onco = models.LsiModel.load(new_path+\"cancer_CUB.lsi\")\n",
    "corpus_CUB_onco = corpora.MmCorpus(new_path+\"cancer_CUB.mm\")\n",
    "\n",
    "# аутоимунные куб\n",
    "dictionary_CUB_aiz = corpora.Dictionary.load(new_path+\"aiz_CUB.dict\")\n",
    "lsi_CUB_aiz = models.LsiModel.load(new_path+\"aiz_CUB.lsi\")\n",
    "corpus_CUB_aiz = corpora.MmCorpus(new_path+\"aiz_CUB.mm\")\n",
    "\n",
    "# инфекции куб \n",
    "dictionary_CUB_inf = corpora.Dictionary.load(new_path+\"infect_CUB.dict\")\n",
    "lsi_CUB_inf = models.LsiModel.load(new_path+\"infect_CUB.lsi\")\n",
    "corpus_CUB_inf = corpora.MmCorpus(new_path+\"infect_CUB.mm\")\n",
    "\n",
    "# офтальмология куб\n",
    "dictionary_CUB_eye = corpora.Dictionary.load(new_path+\"eye_CUB.dict\")\n",
    "lsi_CUB_eye = models.LsiModel.load(new_path+\"eye_CUB.lsi\")\n",
    "corpus_CUB_eye = corpora.MmCorpus(new_path+\"eye_CUB.mm\")\n",
    "\n",
    "# гететические куб\n",
    "dictionary_CUB_gene = corpora.Dictionary.load(new_path+\"gene_CUB.dict\")\n",
    "lsi_CUB_gene = models.LsiModel.load(new_path+\"gene_CUB.lsi\")\n",
    "corpus_CUB_gene = corpora.MmCorpus(new_path+\"gene_CUB.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])          #Indexes\n",
    "index_CUB_chem = similarities.MatrixSimilarity(lsi_CUB_onco[corpus_CUB_chem])\n",
    "index_CUB_onco = similarities.MatrixSimilarity(lsi_CUB_onco[corpus_CUB_onco])\n",
    "index_CUB_aiz = similarities.MatrixSimilarity(lsi_CUB_aiz[corpus_CUB_aiz])\n",
    "index_CUB_inf = similarities.MatrixSimilarity(lsi_CUB_aiz[corpus_CUB_inf])\n",
    "index_CUB_eye = similarities.MatrixSimilarity(lsi_CUB_aiz[corpus_CUB_eye])\n",
    "index_CUB_gene = similarities.MatrixSimilarity(lsi_CUB_aiz[corpus_CUB_gene])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "soup = BeautifulSoup(df.iloc[1,]['summary'], 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fate of the carbon from degraded pollutants in biofiltration is not well understood. The issue of missing carbon needs to be addressed quantitatively to better understand and model biofilter performance. Elucidating the various carbon end-points in various phases should contribute to the fundamental understanding of the degradation kinetics and metabolic pathways as a function of various environmental parameters. This article reviews the implications of key environmental parameters on the carbon end-points. Various studies are evaluated reporting carbon recovery over a multitude of parameters and operational conditions with respect to the analytical measurements and reported distribution of the carbon end-points.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('div p')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fate of the carbon from degraded pollutants in biofiltration is not well understood. The issue of missing carbon needs to be addressed quantitatively to better understand and model biofilter performance. Elucidating the various carbon end-points in various phases should contribute to the fundamental understanding of the degradation kinetics and metabolic pathways as a function of various environmental parameters. This article reviews the implications of key environmental parameters on the carbon end-points. Various studies are evaluated reporting carbon recovery over a multitude of parameters and operational conditions with respect to the analytical measurements and reported distribution of the carbon end-points.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('div p')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enhancment_words import cancerdict, aizdict, infectdict, eyedict, genedict, chemdict\n",
    "def FirstLevelTags(articlerecord, cancerdict=cancerdict, aizdict=aizdict, infectdict=infectdict, eyedict=eyedict, genedict=genedict, chemdict=chemdict, txt=''):\n",
    "    \n",
    "    #articlerecord is a list looking like [journal, title, abstract, doi, altmetrics, topic, chemscore, score]\n",
    "\n",
    "    cancer_idlist, aiz_idlist, infect_idlist, eye_idlist, gene_idlist, chem_idlist = [], [], [], [], [], []\n",
    "    cancer_multiplier, aiz_multiplier, infect_multiplier, eye_multiplier, gene_multiplier, chem_multiplier = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    txt = articlerecord[2].lower()\n",
    "\n",
    "    words = corpora.Dictionary([txt.split()])\n",
    "    ids = words.token2id\n",
    "\n",
    "    # Checking the presence of the target words from our dictionaries in text and adding them to corresponding lists\n",
    "    for x in ids.keys():\n",
    "        for elem in cancerdict:\n",
    "            if x == elem:\n",
    "                cancer_idlist.append(ids[x])\n",
    "        for elem in aizdict:\n",
    "            if x == elem:\n",
    "                aiz_idlist.append(ids[x])\n",
    "        for elem in infectdict:\n",
    "            if x == elem:\n",
    "                infect_idlist.append(ids[x])\n",
    "        for elem in eyedict:\n",
    "            if x == elem:\n",
    "                eye_idlist.append(ids[x])\n",
    "        for elem in genedict:\n",
    "            if x == elem:\n",
    "                gene_idlist.append(ids[x])\n",
    "        for elem in chemdict:\n",
    "            if x == elem:\n",
    "                chem_idlist.append(ids[x])\n",
    "    words = [words.doc2bow(text) for text in [txt.split()]]\n",
    "\n",
    "    # Calculating the amount of target words in the text\n",
    "    for e in words[0]:\n",
    "        for i in cancer_idlist:\n",
    "            if e[0] == i:\n",
    "                cancer_multiplier += e[1]\n",
    "        for i in aiz_idlist:\n",
    "            if e[0] == i:\n",
    "                aiz_multiplier += e[1]\n",
    "        for i in infect_idlist:\n",
    "            if e[0] == i:\n",
    "                infect_multiplier += e[1]\n",
    "        for i in eye_idlist:\n",
    "            if e[0] == i:\n",
    "                eye_multiplier += e[1]\n",
    "        for i in gene_idlist:\n",
    "            if e[0] == i:\n",
    "                gene_multiplier += e[1]\n",
    "        for i in chem_idlist:\n",
    "            if e[0] == i:\n",
    "                chem_multiplier += e[1]\n",
    "\n",
    "    # Constructing a list of amounts of target words for each topic\n",
    "    multipliers = [cancer_multiplier, aiz_multiplier, infect_multiplier, eye_multiplier, gene_multiplier]\n",
    "\n",
    "    #Now if we have a long abstact we will try to make LSI magic upon it. Else, we will use keywords approach\n",
    "    #On this step we try to classify our articles by five medical topics + chemical theme. So, using 2 models: 5-in-1 biological one and chemical one\n",
    "    if len(txt.split()) >= 50:\n",
    "        txt = [x for x in txt if x not in stoplist]\n",
    "        txt = lemmatization(str(txt), allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "        vec_bow = dictionary.doc2bow(txt.split())\n",
    "        vec_bow_chem = dictionary_CUB_chem.doc2bow(txt.split())\n",
    "\n",
    "        vec_lsi = lsi[vec_bow]      # convert the query to LSI space\n",
    "        vec_lsi_chem = lsi_CUB_chem[vec_bow_chem]\n",
    "\n",
    "        sims = index[vec_lsi]       # perform a similarity query against the corpus\n",
    "        sims_chem = index[vec_lsi_chem]\n",
    "        comp_keyword_multiplied = [x*(1+0.08*y) for x, y in zip(sims, multipliers)]   # multiplying the Gensim result to the multiplier from previous steps\n",
    "        sims = list(enumerate(comp_keyword_multiplied))\n",
    "\n",
    "        # Tagging the text with one of the first-level-tags basing on the treshold\n",
    "        number, value = max(sims, key=lambda item: item[1])\n",
    "        if number == 0 and value >= 0.65:  # Pay attentions to the treshold: it is a matter of choice. It is 0.65 now\n",
    "            articlerecord.append(\"cancer\")\n",
    "        elif number == 1 and value >= 0.65:\n",
    "            articlerecord.append(\"autoimmunity\")\n",
    "        elif number == 2 and value >= 0.65:\n",
    "            articlerecord.append(\"infectious\")\n",
    "        elif number == 3 and value >= 0.65:\n",
    "            articlerecord.append(\"ophtalmological\")\n",
    "        elif number == 4 and value >= 0.65:\n",
    "            articlerecord.append(\"gene\")\n",
    "        else:\n",
    "            articlerecord.append(\"uncategorized\")\n",
    "\n",
    "        #calculating the chemical relation\n",
    "        score_chem = sum(sims_chem)\n",
    "\n",
    "    #The keywords approach to title\n",
    "    else:\n",
    "        txt = articlerecord[1].lower()\n",
    "        words = corpora.Dictionary([txt.split()])\n",
    "        ids = words.token2id\n",
    "\n",
    "        # Checking the presence of the target words from our dictionaries in text and adding them to corresponding lists\n",
    "        for x in ids.keys():\n",
    "            for elem in cancerdict:\n",
    "                if x == elem:\n",
    "                    cancer_idlist.append(ids[x])\n",
    "            for elem in aizdict:\n",
    "                if x == elem:\n",
    "                    aiz_idlist.append(ids[x])\n",
    "            for elem in infectdict:\n",
    "                if x == elem:\n",
    "                    infect_idlist.append(ids[x])\n",
    "            for elem in eyedict:\n",
    "                if x == elem:\n",
    "                    eye_idlist.append(ids[x])\n",
    "            for elem in genedict:\n",
    "                if x == elem:\n",
    "                    gene_idlist.append(ids[x])\n",
    "            for elem in chemdict:\n",
    "                if x == elem:\n",
    "                    chem_idlist.append(ids[x])\n",
    "        words = [words.doc2bow(text) for text in [txt.split()]]\n",
    "\n",
    "        # Calculating the amount of target words in the text\n",
    "        for e in words[0]:\n",
    "            for i in cancer_idlist:\n",
    "                if e[0] == i:\n",
    "                    cancer_multiplier += e[1]\n",
    "            for i in aiz_idlist:\n",
    "                if e[0] == i:\n",
    "                    aiz_multiplier += e[1]\n",
    "            for i in infect_idlist:\n",
    "                if e[0] == i:\n",
    "                    infect_multiplier += e[1]\n",
    "            for i in eye_idlist:\n",
    "                if e[0] == i:\n",
    "                    eye_multiplier += e[1]\n",
    "            for i in gene_idlist:\n",
    "                if e[0] == i:\n",
    "                    gene_multiplier += e[1]\n",
    "            for i in chem_idlist:\n",
    "                if e[0] == i:\n",
    "                    chem_multiplier += e[1]\n",
    "\n",
    "        # Constructing a list of amounts of target words for each topic\n",
    "        multipliers = [cancer_multiplier, aiz_multiplier, infect_multiplier, eye_multiplier, gene_multiplier, chem_multiplier]\n",
    "        multipliers = list(enumerate(multipliers))\n",
    "        number, value = max(multipliers, key=lambda item: item[1])\n",
    "\n",
    "        if number == 0:\n",
    "            articlerecord.append(\"cancer\")\n",
    "        elif number == 1:\n",
    "            articlerecord.append(\"autoimmunity\")\n",
    "        elif number == 2:\n",
    "            articlerecord.append(\"infectious\")\n",
    "        elif number == 3:\n",
    "            articlerecord.append(\"ophtalmological\")\n",
    "        elif number == 4:\n",
    "            articlerecord.append(\"gene\")\n",
    "        else:\n",
    "            articlerecord.append(\"uncategorized\")\n",
    "\n",
    "        score_chem = chem_multiplier\n",
    "\n",
    "    articlerecord.append(str(score_chem)+' based on title')\n",
    "    return articlerecord, txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
