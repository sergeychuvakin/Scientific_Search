{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=u\"main.log\", format=u'%(filename)s[LINE:%(lineno)d]# %(levelname)-8s [%(asctime)s]  %(message)s', level=logging.DEBUG)\n",
    "\n",
    "logging.debug( u'debug' )\n",
    "logging.info( u'info' )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import time\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import json\n",
    "# #import Levenshtein as lv\n",
    "# import numpy as np\n",
    "\n",
    "# import spacy\n",
    "\n",
    "\n",
    "# #for findDOI\n",
    "# import feedparser\n",
    "# import jellyfish\n",
    "# import copy\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "import numpy as np\n",
    "import operator as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rss import rss # список рассылки\n",
    "from findDOI import findDOI # функция поиска doi по названию\n",
    "from ALT import altmetrics # функция поиска альтметрик (нужно понять, какие альтметрики нам нужны)\n",
    "from stoplist import stoplist #стоп-лист\n",
    "from rssParser import Parser # класс, который парсит рсс, в датафрейм, - предлагаю его использовать для наполненния\n",
    "from findJournal import findJournal # функция поиска названия журнала по названию статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = Parser(rss)\n",
    "# df = p.main()\n",
    "#df.to_csv('example_table.csv')\n",
    "df = pd.read_csv('example_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>article_name</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Advances in thermochemical conversion of woody...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Fate of degraded pollutants in waste gas biofi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Strain engineering for microbial production of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The state-of-the-art strategies of protein eng...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Engineering of filamentous fungi for efficient...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Publication date: July–August 2019&lt;/p&gt;&lt;p&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       article_name  doi journal  \\\n",
       "0           0  Advances in thermochemical conversion of woody...  NaN     NaN   \n",
       "1           1  Fate of degraded pollutants in waste gas biofi...  NaN     NaN   \n",
       "2           2  Strain engineering for microbial production of...  NaN     NaN   \n",
       "3           3  The state-of-the-art strategies of protein eng...  NaN     NaN   \n",
       "4           4  Engineering of filamentous fungi for efficient...  NaN     NaN   \n",
       "\n",
       "                                             summary  \n",
       "0  <p>Publication date: July–August 2019</p><p><b...  \n",
       "1  <p>Publication date: July–August 2019</p><p><b...  \n",
       "2  <p>Publication date: July–August 2019</p><p><b...  \n",
       "3  <p>Publication date: July–August 2019</p><p><b...  \n",
       "4  <p>Publication date: July–August 2019</p><p><b...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Biotechnology Advances'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "findJournal(df.loc[1,][\"article_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', parser=False, ner=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = '/home/BIOCAD/chuvakin/serge/science_search/pubmed_model/'\n",
    "new_path_CUB = '/home/BIOCAD/chuvakin/serge/science_search/CUB_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем словарь, векторное пространство, модель Lsi (вектороное пространство)\n",
    "# общий корпус натренированный на pubmed \n",
    "dictionary = corpora.Dictionary.load(new_path+'pubmed5.dict')      #Here and later - the first one is PubMed-based LSI-object, other - CUB-based LSI-objects\n",
    "corpus = corpora.MmCorpus(new_path+'pubmed5.mm') \n",
    "lsi = models.LsiModel.load(new_path+'pubmed5.lsi')\n",
    "\n",
    "# химики куб\n",
    "dictionary_CUB_chem = corpora.Dictionary.load(new_path_CUB+\"chem_CUB.dict\")\n",
    "lsi_CUB_chem = models.LsiModel.load(new_path_CUB+\"chem_CUB.lsi\")\n",
    "corpus_CUB_chem = corpora.MmCorpus(new_path_CUB+\"chem_CUB.mm\")\n",
    "\n",
    "# онко куб\n",
    "dictionary_CUB_onco = corpora.Dictionary.load(new_path_CUB+\"cancer_CUB.dict\")\n",
    "lsi_CUB_onco = models.LsiModel.load(new_path_CUB+\"cancer_CUB.lsi\")\n",
    "corpus_CUB_onco = corpora.MmCorpus(new_path_CUB+\"cancer_CUB.mm\")\n",
    "\n",
    "# аутоимунные куб\n",
    "dictionary_CUB_aiz = corpora.Dictionary.load(new_path_CUB+\"aiz_CUB.dict\")\n",
    "lsi_CUB_aiz = models.LsiModel.load(new_path_CUB+\"aiz_CUB.lsi\")\n",
    "corpus_CUB_aiz = corpora.MmCorpus(new_path_CUB+\"aiz_CUB.mm\")\n",
    "\n",
    "# инфекции куб \n",
    "dictionary_CUB_inf = corpora.Dictionary.load(new_path_CUB+\"infect_CUB.dict\")\n",
    "lsi_CUB_inf = models.LsiModel.load(new_path_CUB+\"infect_CUB.lsi\")\n",
    "corpus_CUB_inf = corpora.MmCorpus(new_path_CUB+\"infect_CUB.mm\")\n",
    "\n",
    "# офтальмология куб\n",
    "dictionary_CUB_eye = corpora.Dictionary.load(new_path_CUB+\"eye_CUB.dict\")\n",
    "lsi_CUB_eye = models.LsiModel.load(new_path_CUB+\"eye_CUB.lsi\")\n",
    "corpus_CUB_eye = corpora.MmCorpus(new_path_CUB+\"eye_CUB.mm\")\n",
    "\n",
    "# гететические куб\n",
    "dictionary_CUB_gene = corpora.Dictionary.load(new_path_CUB+\"gene_CUB.dict\")\n",
    "lsi_CUB_gene = models.LsiModel.load(new_path_CUB+\"gene_CUB.lsi\")\n",
    "corpus_CUB_gene = corpora.MmCorpus(new_path_CUB+\"gene_CUB.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])          #Indexes\n",
    "index_CUB_chem = similarities.MatrixSimilarity(lsi_CUB_chem[corpus_CUB_chem])\n",
    "index_CUB_onco = similarities.MatrixSimilarity(lsi_CUB_onco[corpus_CUB_onco])\n",
    "index_CUB_aiz = similarities.MatrixSimilarity(lsi_CUB_aiz[corpus_CUB_aiz])\n",
    "index_CUB_inf = similarities.MatrixSimilarity(lsi_CUB_inf[corpus_CUB_inf])\n",
    "index_CUB_eye = similarities.MatrixSimilarity(lsi_CUB_eye[corpus_CUB_eye])\n",
    "index_CUB_gene = similarities.MatrixSimilarity(lsi_CUB_gene[corpus_CUB_gene])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.231*\"crc\" + 0.174*\"bnab\" + 0.167*\"acpa\" + 0.159*\"plhiv\" + 0.158*\"glaucoma\" + 0.155*\"sarcoidosis\" + 0.155*\"mtx\" + 0.154*\"hbx\" + 0.149*\"gluten\" + 0.142*\"rgcs\"')\n",
      "(1, '0.727*\"fviii\" + 0.316*\"hemophilia\" + 0.277*\"utrophin\" + 0.267*\"grmd\" + 0.192*\"smn2\" + 0.142*\"rfviia\" + 0.137*\"pwh\" + 0.111*\"ahus\" + 0.080*\"hemophiliac\" + 0.074*\"sarcolemma\"')\n",
      "(2, '0.307*\"crc\" + -0.255*\"glaucoma\" + -0.228*\"rgcs\" + -0.183*\"iop\" + 0.167*\"nsclc\" + -0.160*\"rnfl\" + -0.159*\"myopia\" + 0.146*\"ctcs\" + 0.137*\"tcga\" + -0.134*\"myopic\"')\n",
      "(3, '0.232*\"acpa\" + -0.220*\"glaucoma\" + 0.208*\"sarcoidosis\" + 0.206*\"gluten\" + 0.202*\"mtx\" + -0.200*\"crc\" + -0.197*\"rgcs\" + 0.181*\"tnfi\" + -0.159*\"iop\" + 0.146*\"dmard\"')\n",
      "(4, '0.256*\"crc\" + -0.255*\"bnab\" + -0.230*\"plhiv\" + -0.204*\"hbx\" + -0.145*\"efv\" + 0.145*\"acpa\" + 0.140*\"nsclc\" + -0.139*\"hev\" + -0.138*\"nnrti\" + -0.135*\"vpr\"')\n",
      "\n",
      "0) глаза\n",
      "1) онко\n",
      "2) геннетические\n",
      "3) аутоимунные\n",
      "4) инфекции\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[print(i) for i in lsi.print_topics()]\n",
    "\n",
    "print(\n",
    "'''\n",
    "0) глаза\n",
    "1) онко\n",
    "2) геннетические\n",
    "3) аутоимунные\n",
    "4) инфекции\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "soup = BeautifulSoup(df.iloc[1,]['summary'], 'html.parser')\n",
    "article = soup.select('div p')[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare some functions\n",
    "def lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    sentence = []\n",
    "\n",
    "    doc_lem = nlp(text)\n",
    "    for token in doc_lem:\n",
    "        if token.pos_ in allowed_postags:\n",
    "            sentence.append(token.lemma_)\n",
    "        else:\n",
    "            sentence.append(token)\n",
    "\n",
    "    docs_out = \" \".join(str(i).lower() for i in sentence)\n",
    "    docs_out = re.sub(r' - ', '-', docs_out, flags=re.I)\n",
    "    docs_out = re.sub(r'[^A-Za-z0-9\\-]', ' ', docs_out)\n",
    "    docs_out = re.sub(r' \\d+ ', ' ', docs_out)\n",
    "    docs_out = re.sub(r'\\s+', ' ', docs_out)\n",
    "\n",
    "    del doc_lem\n",
    "    return docs_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enhancment_words import cancerdict, aizdict, infectdict, eyedict, genedict, chemdict\n",
    "\n",
    "article = soup.select('div p')[0].text\n",
    "\n",
    "def scores_themes(txt, cancerdict=cancerdict, aizdict=aizdict, infectdict=infectdict, eyedict=eyedict, genedict=genedict, chemdict=chemdict, short=False):\n",
    "    '''\n",
    "    Note that the order of multipliers should correspond the order d rule.\n",
    "    \n",
    "    Function returns following values:\n",
    "    \n",
    "    - theme: topic based on cosine similarity\n",
    "    - chem_multiplier: chemical score, which is number of token occurences in chemical dict\n",
    "    - score_pubmed: lsi score, based on svd decomposition\n",
    "    - score_CUB: wierd number, seems to be constant all the time \n",
    "    - theme_lsi: theme based in svd decomposition\n",
    "    \n",
    "    NB: lsi model returns empty list on too short texts, therefore don't forget to check short=True\n",
    "    \n",
    "    TODO: remove extra scores. Focus on just appropriate values. Right order of themes. \n",
    "    '''\n",
    "    assert type(txt)==str, 'на вход подается не текст'\n",
    "\n",
    "    txt = txt.lower() # lower register\n",
    "    words = corpora.Dictionary([txt.split()]) # own dictionary\n",
    "    ids = words.token2id # dictionary of tokens \n",
    "\n",
    "    # lists of ids of enhancement words\n",
    "    cancer_idlist = [ids[i] for i in list(set(ids.keys()) & set(cancerdict)) if i in ids]\n",
    "    aiz_idlist = [ids[i] for i in list(set(ids.keys()) & set(aizdict)) if i in ids]\n",
    "    infect_idlist = [ids[i] for i in list(set(ids.keys()) & set(infectdict)) if i in ids]\n",
    "    eye_idlist = [ids[i] for i in list(set(ids.keys()) & set(eyedict)) if i in ids]\n",
    "    gene_idlist = [ids[i] for i in list(set(ids.keys()) & set(genedict)) if i in ids]\n",
    "    chem_idlist = [ids[i] for i in list(set(ids.keys()) & set(chemdict)) if i in ids]\n",
    "\n",
    "    # vectorize article abstract\n",
    "    words = words.doc2bow(txt.split())\n",
    "\n",
    "    # lists of multipliers\n",
    "    cancer_multiplier = sum([e[1] for e in words if e[0] in cancer_idlist])\n",
    "    aiz_multiplier = sum([e[1] for e in words if e[0] in aiz_idlist])\n",
    "    infect_multiplier = sum([e[1] for e in words if e[0] in infect_idlist])\n",
    "    eye_multiplier = sum([e[1] for e in words if e[0] in eye_idlist])\n",
    "    gene_multiplier = sum([e[1] for e in words if e[0] in gene_idlist])\n",
    "    chem_multiplier = sum([e[1] for e in words if e[0] in chem_idlist])\n",
    "\n",
    "    # one multiplier\n",
    "    multipliers = [eye_multiplier, cancer_multiplier ,  gene_multiplier, aiz_multiplier, infect_multiplier]\n",
    "    multipliers = np.array([(1+0.08*y) for y in multipliers])\n",
    "    if short==False:\n",
    "        \n",
    "        # preproc \n",
    "        txt = lemmatization(txt, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "        txt = [x for x in txt.split() if x not in stoplist]\n",
    "\n",
    "        # preporation for model estimation\n",
    "        vec_bow = dictionary.doc2bow(txt)\n",
    "        vec_bow_chem = dictionary_CUB_chem.doc2bow(txt)\n",
    "\n",
    "        # modeling\n",
    "        vec_lsi = lsi[vec_bow]      # convert the query to LSI space\n",
    "        vec_lsi_chem = lsi_CUB_chem[vec_bow_chem]\n",
    "\n",
    "        # find similarities\n",
    "        sims  = index[vec_lsi] * multipliers\n",
    "        sims_chem = index_CUB_chem[vec_lsi_chem] # наверное это лишнее\n",
    "\n",
    "        #  темы \n",
    "        d = dict(zip(['глаза', 'онко', 'геннетические', 'аутоимунные', 'инфекции'], sims))\n",
    "\n",
    "        # find most relevant topic, which is more than 0.65 score similarity \n",
    "        try:\n",
    "            theme, score_pubmed = max(list(filter(lambda x: x[1]>0.65, d.items())), key=op.itemgetter(1))\n",
    "        except: \n",
    "            theme, score_pubmed = 'uncategorized', np.nan\n",
    "\n",
    "        score_chem = sum(sims_chem) # зачем это??\n",
    "\n",
    "        # define theme by lsi_model \n",
    "        theme_lsi, score_lsi = sorted([(x[0],x[1]*y) for x, y in zip(vec_lsi, multipliers)], key=op.itemgetter(1), reverse=True)[0]\n",
    "        theme_lsi = dict(enumerate(d.keys()))[theme_lsi] \n",
    "\n",
    "\n",
    "        # count similirities with cub (Всегда отдает одно число!)\n",
    "        if  theme == \"онко\":\n",
    "            vec_bow_CUB = dictionary_CUB_onco.doc2bow(txt)\n",
    "            vec_lsi_CUB = lsi_CUB_onco[vec_bow_CUB]\n",
    "            sims_CUB = index_CUB_onco[vec_lsi_CUB]\n",
    "        elif theme == \"аутоимунные\":\n",
    "            vec_bow_CUB = dictionary_CUB_aiz.doc2bow(txt)\n",
    "            vec_lsi_CUB = lsi_CUB_aiz[vec_bow_CUB]\n",
    "            sims_CUB = index_CUB_aiz[vec_lsi_CUB]\n",
    "        elif theme == \"инфекции\":\n",
    "            vec_bow_CUB = dictionary_CUB_inf.doc2bow(txt)\n",
    "            vec_lsi_CUB = lsi_CUB_inf[vec_bow_CUB]\n",
    "            sims_CUB = index_CUB_inf[vec_lsi_CUB]\n",
    "        elif theme == \"глаза\":\n",
    "            vec_bow_CUB = dictionary_CUB_eye.doc2bow(txt)\n",
    "            vec_lsi_CUB = lsi_CUB_eye[vec_bow_CUB]\n",
    "            sims_CUB = index_CUB_eye[vec_lsi_CUB]\n",
    "        elif theme == \"геннетические\":\n",
    "            vec_bow_CUB = dictionary_CUB_gene.doc2bow(txt)\n",
    "            vec_lsi_CUB = lsi_CUB_gene[vec_bow_CUB]\n",
    "            sims_CUB = index_CUB_gene[vec_lsi_CUB]\n",
    "        else:\n",
    "            sims_CUB = [0,0,0]\n",
    "\n",
    "        score_CUB = sum(sims_CUB)\n",
    "    \n",
    "    else:\n",
    "        score_CUB = \"too short for similarities\"\n",
    "        theme = max(zip(['глаза', 'онко', 'геннетические', 'аутоимунные', 'инфекции'], multipliers), key=op.itemgetter(1))[0]\n",
    "        theme_lsi = 'undefined'\n",
    "        score_pubmed = 'undefined'\n",
    "    return theme, chem_multiplier, score_pubmed, score_CUB, theme_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('глаза', 0, 0.7084100246429443, 8.0, 'глаза')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scores import *\n",
    "scores_themes(article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
